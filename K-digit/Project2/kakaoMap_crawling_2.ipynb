{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0172445c-15a4-478e-aeb1-79c7e006964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021.08.11\n",
    "# kakaoMap_crawling_1.ipynb 수정버전\n",
    "# 수정 내용 : 리뷰가 5페이지인 경우 4페이지를 중복해서 읽는 경우 해결\n",
    "# 추가 내용 : 각자 맡은 지역구를 리스트로 작성하여 검색 자동화, to_csv 추가\n",
    "\n",
    "# 문제점 : search_area.clear() 작동 오류로 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75993ed-3284-47a6-9519-3c36ff2614f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "##############################################################  ############\n",
    "##################### variable related selenium ##########################\n",
    "##########################################################################\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "\n",
    "driver = webdriver.Chrome('./webdriver/chromedriver')\n",
    "\n",
    "rating_df = pd.DataFrame()\n",
    "restaurant_df = pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    global driver, load_wb, review_num, rating_df, restaurant_df\n",
    "\n",
    "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
    "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
    "\n",
    "     # 검색할 목록\n",
    "#     place_infos = ['종로구 맛집', '중구 맛집', ' 용산구 맛집', '성동구 맛집', '광진구 맛집', '동대문구 맛집', '중랑구 맛집', '성북구 맛집', '강북구 맛집']\n",
    "#     place_infos = ['도봉구 맛집', '노원구 맛집', '은평구 맛집', '서대문구 맛집', '마포구 맛집', '양천구 맛집', '강서구 맛집','구로구 맛집']\n",
    "    place_infos = ['금천구 맛집', '영등포구 맛집', '동작구 맛집', '관악구 맛집', '서초구 맛집', '강남구 맛집', '송파구 맛집','강동구 맛집']\n",
    "    \n",
    "    for i, place in enumerate(place_infos):\n",
    "        print(\"#####\", i,\":\",place)\n",
    "        search(place)\n",
    "        \n",
    "        rating_df.to_csv('%s_rating_df.csv' %place)\n",
    "        rating_df.to_csv('%s_rating_df_ko.csv' %place, sep=',', na_rep='NaN', encoding='utf-8-sig')\n",
    "        \n",
    "        restaurant_df.to_csv('%s_restaurant_df.csv'  %place)\n",
    "        restaurant_df.to_csv('%s_restaurant_df_ko.csv'  %place, sep=',', na_rep='NaN', encoding='utf-8-sig')\n",
    "        \n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER) # 더보기\n",
    "    xPath = '//*[@id=\"info.search.page.no1\"]'\n",
    "    driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "    sleep(1)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(place, place_lists)\n",
    "    search_area.clear()\n",
    "\n",
    "    # 전체 페이지\n",
    "    while True:\n",
    "        try:\n",
    "            # 2~ 5페이지 읽기\n",
    "            for i in range(2, 6):\n",
    "                # 페이지 넘기기\n",
    "                xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "                driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "                \n",
    "                crawling(place, place_lists)\n",
    "                \n",
    "                # 다음 페이지 넘기기\n",
    "                if i==5:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"info.search.page.next\"]').send_keys(Keys.ENTER)\n",
    "\n",
    "        except ElementNotInteractableException:\n",
    "            print('end page')\n",
    "            break\n",
    "            \n",
    "        finally:\n",
    "            search_area.clear()\n",
    "        \n",
    "\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "    \"\"\"\n",
    "    페이지 목록을 받아서 크롤링 하는 함수\n",
    "    :param place: 리뷰 정보 찾을 장소이름\n",
    "    \"\"\"\n",
    "    \n",
    "    global restaurant_df\n",
    "\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "        place_local = place.select('.info_item > .addr > .lot_number')[0].text\n",
    "        place_category = place.select('.head_item > .subcategory')[0].text\n",
    "        place_detail = place.select('.info_item > .contact> .moreview')[0].get('href') # place detail\n",
    "        \n",
    "        #### DataFrame 저장 ####\n",
    "        row = {\"ItemID\":place_name, \"address\": place_address, \"local\" : place_local, \"category\": place_category}\n",
    "        row = pd.DataFrame(row, index=[1])\n",
    "        restaurant_df = restaurant_df.append(row, ignore_index=True)\n",
    "        \n",
    "        #### 데이터 베이스 저장 #####\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        driver.execute_script('window.open(\"about:blank\", \"_blank\");')\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(place_detail) # 상세정보 탭으로 변환\n",
    "        sleep(1)\n",
    "        \n",
    "        print('####', place_name)\n",
    "\n",
    "        # 첫 페이지\n",
    "        extract_review(place_name) # 리뷰 추출\n",
    "\n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "            for i in range(page_num-1):\n",
    "                # css selector를 이용해 페이지 버튼 누르기\n",
    "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                extract_review(place_name)\n",
    "                idx += 1\n",
    "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
    "            \n",
    "            sleep(1)\n",
    "            extract_review(place_name) # 리뷰 추출\n",
    "            \n",
    "            # 그 이후 페이지\n",
    "            while True:\n",
    "                idx = 4\n",
    "                page_num = len(driver.find_elements_by_class_name('link_page')) #페이지 수 찾기\n",
    "                for i in range(page_num-1):\n",
    "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                    sleep(1)\n",
    "                    extract_review(place_name)\n",
    "                    idx += 1\n",
    "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
    "                sleep(1)\n",
    "                extract_review(place_name) # 리뷰 추출            \n",
    "            \n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "\n",
    "def extract_review(place_name):\n",
    "    global driver, rating_df\n",
    "\n",
    "    ret = True\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 첫 페이지 리뷰 목록 찾기\n",
    "    review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "    # 리뷰가 있는 경우\n",
    "    if len(review_lists) != 0:\n",
    "        for i, review in enumerate(review_lists):\n",
    "            comment = review.select('.txt_comment > span') # 리뷰\n",
    "            rating = review.select('.grade_star > em') # 별점\n",
    "            user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "            timestamp = review.select(' div > span.time_write') #시간정보\n",
    "            \n",
    "            val = ''\n",
    "            if len(comment) != 0:\n",
    "                if len(rating) != 0:\n",
    "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
    "                else:\n",
    "                    val = comment[0].text + '/0'\n",
    "                print(val)                      \n",
    "                \n",
    "                try:\n",
    "                    row = {\"ItemID\":place_name, \"UserID\":user_id[0].get('data-userid'), \"review\" : comment[0].text,\n",
    "                           \"Rating\":rating[0].text.replace('점', ''), \"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row, ignore_index=True)\n",
    "            \n",
    "                except:\n",
    "                    row = {\"ItemID\":place_name, \"UserID\":None, \"review\" : None,\n",
    "                           \"Rating\": None, \"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        print('no review in extract')\n",
    "        ret = False\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
