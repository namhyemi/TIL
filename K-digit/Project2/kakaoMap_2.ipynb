{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "854ddfca-2d76-405b-9643-eb9f3fd7b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140d728f-2cf3-4245-80e7-5198e0fb0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d21f07c-efb8-4367-9bab-e8451f9ca06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = 'https://map.kakao.com/'\n",
    "item = '맛집'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fc39cb4-5cca-4457-a2d4-49ad57a7cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('../webdriver/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "603c5317-8561-4a45-b46d-8a265892eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(urls) # 카카오 지도 접속하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "970281ae-5e3e-448c-816c-6422e05786ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchbox = driver.find_element_by_xpath(\"//input[@id='search.keyword.query']\") # 검색창에 입력하기 \n",
    "searchbox.send_keys(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81025abf-bf8c-4bb4-91be-fe2e8177e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchbutton = driver.find_element_by_xpath(\"//button[@id='search.keyword.submit']\") # 검색버튼 누르기\n",
    "driver.execute_script(\"arguments[0].click();\", searchbutton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31195f67-d479-45a0-9408-f7568f58863e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 124)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m124\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "if len(driver.find_elements_by_xpath(\"//a[@class='moreview']\")) != 0:\n",
    "        print('식당 존재')\n",
    "        driver.execute_script('window.open(\"about:blank\", \"_blank\");') # 새 탭 열기\n",
    "        reviewbutton = driver.find_element_by_xpath(\"//a[@class='numberofscore']\")\n",
    "        time.sleep(2)\n",
    "        content_url = reviewbutton.get_attribute(\"href\") \n",
    "        tabs = driver.window_handles\n",
    "        driver.switch_to_window(tabs[1]) # 새 탭으로 이동\n",
    "        driver.get(content_url) # 링크 열기\n",
    "        time.sleep(3)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_lists = soup.select('.list_evaluation > li')\n",
    "        print(len(review_lists))\n",
    "        if len(review_lists) != 0 :\n",
    "            for i, review in enumerate(review_lists) :\n",
    "                user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                rating = review.select('.grade_star > em') # 별점\n",
    "                try:\n",
    "                    img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                except:\n",
    "                    continue\n",
    "                user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                try:\n",
    "                    row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":None,\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                \n",
    "                    \n",
    "        else :\n",
    "            print(\"리뷰가 없습니다\")\n",
    "            \n",
    "        try:\n",
    "            for i in range(2,500):\n",
    "                time.sleep(3)\n",
    "                another_review = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                another_review.click()\n",
    "                time.sleep(3)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                if len(review_lists) != 0 :\n",
    "                    for i, review in enumerate(review_lists) :\n",
    "                        user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                        rating = review.select('.grade_star > em') # 별점\n",
    "                        try:\n",
    "                            img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                        except:\n",
    "                            continue\n",
    "                        user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                        timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                        try:\n",
    "                            row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'), \"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            time.sleep(1)\n",
    "                            \n",
    "                        except:\n",
    "                            row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            \n",
    "                        \n",
    "                            \n",
    "        except:\n",
    "            print(\"더 이상 리뷰 존재 X\")\n",
    "            driver.close()\n",
    "        driver.switch_to_window(tabs[0])\n",
    "        print(\"기본 페이지로 돌아가자\")\n",
    "            \n",
    "    else:\n",
    "        print(\"식당 존재 x\")\n",
    "        \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934bff0-1d59-413d-84ae-111bee6ee9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_list = []\n",
    "userid_list = []\n",
    "itemid_list = []\n",
    "timestamp_list = []\n",
    "comment_list = []\n",
    "\n",
    "for k, row in nowon.iterrows():\n",
    "    url = nowon[\"kakao_url\"][k]\n",
    "    print(k,\"번째 크롤링 중 : \",nowon[\"상호명\"][k] , sep=\" \")\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        rating_num = driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_sorting > a > span.color_b').text\n",
    "        rating_num = int(rating_num)\n",
    "        print(rating_num)\n",
    "        \n",
    "        # 별점 개수로 페이지를 넘기는 기준을 정하기로 함 / 한 페이지 당 최대 리뷰는 5개!\n",
    "        if rating_num > 0 and rating_num <= 5:\n",
    "            p_num = 1\n",
    "            print(p_num)\n",
    "            \n",
    "            time.sleep(2)\n",
    "           \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "            review_lists = soup.select('.list_evaluation > li')\n",
    "            time.sleep(3)\n",
    "            for j in range(0, len(user)):\n",
    "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                itemid_list.append(nowon[\"상호명\"][k])\n",
    "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "                time.sleep(3)\n",
    "            if len(review_lists) != 0:\n",
    "                for j, review in enumerate(review_lists):\n",
    "                    comment = review.select('.txt_comment > span') \n",
    "                    rating = review.select('.grade_star > em') # 별점\n",
    "                            \n",
    "                    if len(comment) != 0:\n",
    "                        if len(rating) != 0 :\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append(rating[0].text)\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append('0')\n",
    "            else:\n",
    "                print('리뷰가 없습니다.')\n",
    "                time.sleep(3)    \n",
    "                    \n",
    "                    \n",
    "        # 2 페이지\n",
    "        elif rating_num >= 6 and rating_num <= 10:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "            review_lists = soup.select('.list_evaluation > li')\n",
    "            time.sleep(3)\n",
    "            for j in range(0, len(user)):\n",
    "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                itemid_list.append(nowon[\"상호명\"][k])\n",
    "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "                time.sleep(3)\n",
    "            if len(review_lists) != 0:\n",
    "                for j, review in enumerate(review_lists):\n",
    "                    comment = review.select('.txt_comment > span') \n",
    "                    rating = review.select('.grade_star > em') # 별점\n",
    "                            \n",
    "                    if len(comment) != 0:\n",
    "                        if len(rating) != 0:\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append(rating[0].text)\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append('0')\n",
    "            else:\n",
    "                print('리뷰가 없습니다.')\n",
    "                time.sleep(3)\n",
    "                    \n",
    "            driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a').click()\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "            review_lists = soup.select('.list_evaluation > li')\n",
    "            time.sleep(3)\n",
    "            \n",
    "            for j in range(0, len(user)):\n",
    "                userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                itemid_list.append(nowon[\"상호명\"][k])\n",
    "                timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "                time.sleep(3)\n",
    "            if len(review_lists) != 0:\n",
    "                for j, review in enumerate(review_lists):\n",
    "                    comment = review.select('.txt_comment > span') \n",
    "                    rating = review.select('.grade_star > em') # 별점\n",
    "                            \n",
    "                    if len(comment) != 0:\n",
    "                        if len(rating) != 0:\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append(rating[0].text)\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            comment_list.append(comment[0].text) \n",
    "                            rating_list.append('0')\n",
    "            else:\n",
    "                print('리뷰가 없습니다.')\n",
    "                time.sleep(3)\n",
    "                    \n",
    "        # 3페이지 이하         \n",
    "        elif rating_num >= 11 and rating_num <= 15:\n",
    "            p_num = 3\n",
    "            print(p_num)\n",
    "                 \n",
    "            for i in range(1,p_num+1):\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                time.sleep(3)\n",
    "                for j in range(0, len(user)):\n",
    "                    userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                    itemid_list.append(nowon[\"상호명\"][k])\n",
    "                    timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "               \n",
    "            # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
    "                    time.sleep(3)\n",
    "                if len(review_lists) != 0:\n",
    "                    for j, review in enumerate(review_lists):\n",
    "                        comment = review.select('.txt_comment > span') \n",
    "                        rating = review.selec('.grade_star > em')\n",
    "                            \n",
    "                        if len(comment) != 0:\n",
    "                            if len(rating) != 0:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append(rating[0].text)\n",
    "                                time.sleep(3)\n",
    "                            else:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append('0')\n",
    "                else:\n",
    "                    print('리뷰가 없습니다.')\n",
    "                try: # 마지막 페이지 크롤링을 위해서 try사용\n",
    "                    driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
    "                except:\n",
    "                    continue\n",
    "                time.sleep(2)\n",
    "               \n",
    "                \n",
    "        # 4페이지     \n",
    "        elif rating_num >= 16 and rating_num <= 20:\n",
    "            p_num = 4\n",
    "            print(p_num)\n",
    "            \n",
    "            for i in range(1,p_num+1):\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                time.sleep(2)\n",
    "                for j in range(0, len(user)):\n",
    "                    userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                    itemid_list.append(nowon[\"상호명\"][k])\n",
    "                    timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "            # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
    "                time.sleep(2)\n",
    "               \n",
    "                if len(review_lists) != 0:\n",
    "                    for j, review in enumerate(review_lists):\n",
    "                        comment = review.select('.txt_comment > span') \n",
    "                        rating = review.select('.grade_star > em')\n",
    "                            \n",
    "                        if len(comment) != 0:\n",
    "                            if len(rating) != 0:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append(rating[0].text)\n",
    "                                time.sleep(3)\n",
    "                            else:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append('0')\n",
    "                else:\n",
    "                    print('리뷰가 없습니다.')\n",
    "                try: # 마지막 페이지 크롤링을 위해서 try사용\n",
    "                    driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
    "                except:\n",
    "                    continue\n",
    "                time.sleep(2)\n",
    "            \n",
    "                        \n",
    "            \n",
    "                        \n",
    "        # 5페이지 이상           \n",
    "        elif rating_num >= 21:\n",
    "            p_num = int(math.ceil(rating_num / 5))\n",
    "            print(p_num)\n",
    "            \n",
    "            for i in range(1, 7):\n",
    "                p_num -= 1\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                time.sleep(2)\n",
    "                for j in range(0, len(user)):\n",
    "                    userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                    itemid_list.append(nowon[\"상호명\"][k])\n",
    "                    timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "             # 카카오맵의 개인 user별 url의 규칙은 다음과 같습니다.\n",
    "                    time.sleep(3)\n",
    "              \n",
    "                \n",
    "                    # 리뷰 리스트가 0이 아니면\n",
    "                if len(review_lists) != 0:\n",
    "                    for j, review in enumerate(review_lists):\n",
    "                        comment = review.select('.txt_comment > span') \n",
    "                        rating = review.select('.grade_star > em') # 별점\n",
    "                            \n",
    "                        if len(comment) != 0:\n",
    "                            if len(rating) != 0:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append(rating[0].text)\n",
    "                                time.sleep(3)\n",
    "                            else:\n",
    "                                comment_list.append(comment[0].text) \n",
    "                                rating_list.append('0')\n",
    "                else:\n",
    "                    print('리뷰가 없습니다.')\n",
    "                try: # 마지막 페이지 크롤링을 위해서 try사용\n",
    "                    driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(i) +']').click() # 페이지별로 실행하고\n",
    "                except:\n",
    "                    continue\n",
    "                time.sleep(2)\n",
    "                print(p_num)\n",
    "                pp_num = p_num\n",
    "            while p_num <= pp_num and p_num >= 0:\n",
    "                for a in range(2,7) :\n",
    "                    p_num -= 1\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    review_lists = soup.select('.list_evaluation > li')\n",
    "                    user = driver.find_elements_by_xpath(\"//a[@class='link_user']\")\n",
    "                    time.sleep(2)\n",
    "                    for j in range(0,len(user)):\n",
    "                        userid_list.append(user[j].get_attribute(\"data-userid\")) # 새로 생성한 userid_list에 userid를 추출해 더합니다.\n",
    "                        itemid_list.append(nowon[\"상호명\"][k])\n",
    "                        timestamp_list.append(soup.select('.time_write')[j].text)\n",
    "                        time.sleep(2)\n",
    "                    if len(review_lists) != 0:\n",
    "                        for i, review in enumerate(review_lists):\n",
    "                            comment = review.select('.txt_comment > span') # 리뷰\n",
    "                            rating = review.select('.grade_star > em') # 별점\n",
    "                            if len(comment) != 0:\n",
    "                                if len(rating) != 0:\n",
    "                                    comment_list.append(comment[0].text) \n",
    "                                    rating_list.append(rating[0].text)\n",
    "                                    time.sleep(3)\n",
    "                                else:\n",
    "                                    comment_list.append(comment[0].text) \n",
    "                                    rating_list.append('0')\n",
    "                    else:\n",
    "                        print('no review in extract')\n",
    "                    try:\n",
    "                        driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[4]/div[4]/div/a['+ str(a) +']').click() # 페이지별로 실행하고\n",
    "                    except:\n",
    "                        continue\n",
    "                    time.sleep(3)    \n",
    "              \n",
    "            \n",
    "                            \n",
    "                                       \n",
    "\n",
    "    except (NoSuchElementException, ElementNotInteractableException):\n",
    "        print(\"리뷰가 없습니다\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39d06e-9ae6-4b24-86ac-94b07ee40c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "current = 0\n",
    "goal = len(items)\n",
    "\n",
    "for item in items :\n",
    "    current += 1\n",
    "    print('진행상황 : ', current,'/',goal,sep=\"\")\n",
    "    # 리뷰가 없을 때의 코드\n",
    "    driver.get(\"https://map.kakao.com/\") # 카카오 지도 접속하기\n",
    "    searchbox = driver.find_element_by_xpath(\"//input[@id='search.keyword.query']\") # 검색창에 입력하기\n",
    "    searchbox.send_keys(item)\n",
    "    time.sleep(2)\n",
    "    searchbutton = driver.find_element_by_xpath(\"//button[@id='search.keyword.submit']\") # 검색버튼 누르기\n",
    "    driver.execute_script(\"arguments[0].click();\", searchbutton)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if len(driver.find_elements_by_xpath(\"//a[@class='moreview']\")) != 0:\n",
    "        print('식당 존재')\n",
    "        driver.execute_script('window.open(\"about:blank\", \"_blank\");') # 새 탭 열기\n",
    "        reviewbutton = driver.find_element_by_xpath(\"//a[@class='numberofscore']\")\n",
    "        time.sleep(2)\n",
    "        content_url = reviewbutton.get_attribute(\"href\") \n",
    "        tabs = driver.window_handles\n",
    "        driver.switch_to_window(tabs[1]) # 새 탭으로 이동\n",
    "        driver.get(content_url) # 링크 열기\n",
    "        time.sleep(3)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_lists = soup.select('.list_evaluation > li')\n",
    "        print(len(review_lists))\n",
    "        if len(review_lists) != 0 :\n",
    "            for i, review in enumerate(review_lists) :\n",
    "                user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                rating = review.select('.grade_star > em') # 별점\n",
    "                try:\n",
    "                    img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                except:\n",
    "                    continue\n",
    "                user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                try:\n",
    "                    row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":None,\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                \n",
    "                    \n",
    "        else :\n",
    "            print(\"리뷰가 없습니다\")\n",
    "            \n",
    "        try:\n",
    "            for i in range(2,500):\n",
    "                time.sleep(3)\n",
    "                another_review = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                another_review.click()\n",
    "                time.sleep(3)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                if len(review_lists) != 0 :\n",
    "                    for i, review in enumerate(review_lists) :\n",
    "                        user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                        rating = review.select('.grade_star > em') # 별점\n",
    "                        try:\n",
    "                            img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                        except:\n",
    "                            continue\n",
    "                        user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                        timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                        try:\n",
    "                            row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'), \"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            time.sleep(1)\n",
    "                            \n",
    "                        except:\n",
    "                            row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            \n",
    "                        \n",
    "                            \n",
    "        except:\n",
    "            print(\"더 이상 리뷰 존재 X\")\n",
    "            driver.close()\n",
    "        driver.switch_to_window(tabs[0])\n",
    "        print(\"기본 페이지로 돌아가자\")\n",
    "            \n",
    "    else:\n",
    "        print(\"식당 존재 x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfd0919-235c-46ac-898b-07185b7777bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f701e-5cf4-48de-8fec-158f6ade3da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
